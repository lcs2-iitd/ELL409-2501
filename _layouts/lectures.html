---
layout: page
---
{{ content }}

<ul class="course-plan">
  <li>
    <strong>Course Introduction</strong> – An introduction to the course syllabus, timeline and background.
  </li>
  <li>
    <strong>Decision Tree</strong> – Introduction, Usage, Types, Shannon's Entropy, Information Gain, Examples, ID3, Inductive Bias, Overfitting & its avoiding techniques.
  </li>
  <li>
    <strong>Linear Regression</strong> – MLE, MEP, Introduction to LR, Normal equation, closed-form solution, probabilistic interpretation, locally weighted LR, and error function analysis.
  </li>
  <li>
    <strong>Logistic Regression</strong> – Logistic regression training, sigmoid function, and concave objective function.
  </li>
  <li>
    <strong>Generalized Linear Models (GLMs)</strong> – Exponential family of distributions, link function, and proofs for Normal, Poisson, and Binomial distributions.
  </li>
  <li>
    <strong>Generative Learning Algorithms</strong> – Gaussian Discriminant Analysis, linear/quadratic decision boundaries, Naive Bayes Classifier, and smoothing.
  </li>
  <li>
    <strong>Support Vector Machines (SVMs)</strong> – Coordinate geometry basics, Lagrange multipliers, KKT conditions, primal/dual formulation, SVM optimization, and kernel tricks (Linear, Polynomial, RBF, Soft Margin).
  </li>
  <li>
    <strong>Bias-Variance Tradeoff</strong> – Mathematical derivation of bias, variance, and noise; discussion on their trade-off.
  </li>
  <li>
    <strong>Ensemble Learning</strong> – Bagging, Boosting, Random Forest.
  </li>
  <li>
    <strong>Boosting</strong> – Weak learners, reducing bias, Anyboost, and AdaBoost algorithms.
  </li>
  <li>
    <strong>Instance-Based Learning</strong> – k-Nearest Neighbors (kNN), error analysis, and bias-variance trade-off.
  </li>
  <li>
    <strong>Dimensionality Reduction</strong> – Principal Component Analysis (PCA), Singular Value Decomposition (SVD), and Linear Discriminant Analysis (LDA).
  </li>
  <li>
    <strong>Introduction to Neural Networks</strong> – Perceptron, Multi-layer Perceptron, activation functions, backpropagation, CNNs, and RNNs.
  </li>
  <li>
    <strong>RNN</strong> – Backpropagation through time (BPTT), LSTM, Attention mechanisms, and RNN limitations.
  </li>
  <li>
    <strong>Transformer</strong> – Self-attention, masked self-attention, and encoder-decoder architecture.
  </li>
</ul>

<!--

<ul id="archive">
{% for lecture in site.lectures %}
<li class="archiveposturl" style="background: transparent">
<div class="lecture-container">
    {% if lecture.thumbnail %}
    <div class="thumbnail">
      <div class="center-cropped" style="margin-top:5px;margin-bottom:5px;background-image: url('{{ lecture.thumbnail | prepend: site.baseurl }}');">
        <img src="{{ lecture.thumbnail | prepend: site.baseurl }}"/>
      </div>
    </div>
    {% endif %}
    <div class="content">
        <span style="font-weight: bold;">{{ lecture.title }}</span><br>

        <strong>tl;dr:</strong> {{ lecture.tldr }}
        <br/>
    </div>
</div>
</li>
{% endfor %}
</ul>


<ul id="archive">
{% for lecture in site.lectures %}
<li class="archiveposturl" style="background: transparent">
<div class="lecture-container">
    {% if lecture.thumbnail %}
    <div class="thumbnail">
      <div class="center-cropped" style="margin-top:5px;margin-bottom:5px;background-image: url('{{ lecture.thumbnail | prepend: site.baseurl }}');">
        <img src="{{ lecture.thumbnail | prepend: site.baseurl }}"/>
      </div>
    </div>
    {% endif %}
    <div class="content">
        <span style="font-weight: bold;">{{ lecture.title }}</span><br>

        <strong>tl;dr:</strong> {{ lecture.tldr }}
        <br/>
        <strong>
        {% include lecture_links.html lecture=lecture %}
        </strong>

        {% if lecture.content != '' %}
        <br/>
        <div class="markdown-content" style="margin-top: 3px;">
        {{ lecture.content }}
        </div>
        {% endif %}
    </div>
</div>
</li>
{% endfor %}
</ul>
-->
